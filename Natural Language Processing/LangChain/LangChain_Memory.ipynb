{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUpq0ofbp35i"
      },
      "outputs": [],
      "source": [
        "# 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"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "# !pip install python-dotenv\n",
        "!pip install --upgrade langchain"
      ],
      "metadata": {
        "id": "4hIc_3AFqH3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "DrfCtq7Vul6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "xWTaXb7tqWum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    temperature = 0.0,\n",
        "    openai_api_key= \"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\"\n",
        ")\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3meTxl7qW19",
        "outputId": "e2458d61-c996-40a6-b616-9f4c65d1ddcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory()\n",
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-V7iNk3qW9U",
        "outputId": "73ff42ed-d395-43dd-c92c-5f4936d5e2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "        llm = llm,\n",
        "        memory = memory,\n",
        "        verbose =  True\n",
        ")\n",
        "conversation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmuao2WYwKN7",
        "outputId": "ed43f419-d417-4626-9703-f90502bdacdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationChain(memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history'), callbacks=None, callback_manager=None, verbose=True, tags=None, prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True), llm=ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None), output_key='response', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}, input_key='input')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"Hi, I'm Shashank\")"
      ],
      "metadata": {
        "id": "u6N2y4MXqXEY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "2c03fe03-3687-483b-e2d6-0f84d91df7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, I'm Shashank\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Shashank, it's nice to meet you. I am an AI language model designed to have conversations with humans. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"What is my name and it's meaning?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "Du2VNTPExr7v",
        "outputId": "e3c46ad1-8f98-4ca6-fbc8-6ebaa692f20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, I'm Shashank\n",
            "AI: Hello Shashank, it's nice to meet you. I am an AI language model designed to have conversations with humans. How can I assist you today?\n",
            "Human: What is my name and it's meaning?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Shashank, which is a Sanskrit name meaning \"moon-crested\" or \"one who bears the moon on his head\". It is a popular name in India and is often given to boys.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx8tAh35zCYO",
        "outputId": "6ec657a7-c3f7-43f1-d949-af757f6ac700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Hi, I\\'m Shashank\\nAI: Hello Shashank, it\\'s nice to meet you. I am an AI language model designed to have conversations with humans. How can I assist you today?\\nHuman: What is my name and it\\'s meaning?\\nAI: Your name is Shashank, which is a Sanskrit name meaning \"moon-crested\" or \"one who bears the moon on his head\". It is a popular name in India and is often given to boys.'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wxf3SRRxsIx",
        "outputId": "6778ed7e-ad81-4a94-da9b-a161760c9fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content=\"Hi, I'm Shashank\", additional_kwargs={}, example=False), AIMessage(content=\"Hello Shashank, it's nice to meet you. I am an AI language model designed to have conversations with humans. How can I assist you today?\", additional_kwargs={}, example=False), HumanMessage(content=\"What is my name and it's meaning?\", additional_kwargs={}, example=False), AIMessage(content='Your name is Shashank, which is a Sanskrit name meaning \"moon-crested\" or \"one who bears the moon on his head\". It is a popular name in India and is often given to boys.', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory =  ConversationBufferMemory()\n",
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TLQF2LJxsO4",
        "outputId": "919b477d-4e98-477b-8054-d8fa5afa3930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Hi\"}, {\"output\":\"What's up\"})\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEapzXOZ0Fjj",
        "outputId": "8ae47596-65bb-4e5c-ef9a-141fbaf1474e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv9MgkiI0MhY",
        "outputId": "e1b89b4f-348d-431f-ccd4-eb74a02677e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dCNbh0t-0wtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversation Buffer Window Memory**"
      ],
      "metadata": {
        "id": "Kv-4mxbw01fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "8bP7G_PL0wqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)   # k = 1 specifies that I wanted to remember just one conversational exchange"
      ],
      "metadata": {
        "id": "TiS3eEb70Mn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Hi\"}, {\"output\":\"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much just hanging\"}, {\"output\":\"Cool\"})"
      ],
      "metadata": {
        "id": "0OJGp62b0MvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})          # Remembers the recent communication exchange"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qTB-Z7c1hS1",
        "outputId": "81214726-c51c-4780-9add-3212bf96b786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much just hanging\\nAI: Cool'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DcUiIPNE19G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "conversation = ConversationChain(\n",
        "    llm = llm,\n",
        "    memory = memory,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "IyTp666V19M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"Hi, My name is Shashank\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "Ps5KYZH-2mEb",
        "outputId": "09a2b016-89a8-4f81-f207-23a43174c5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, My name is Shashank\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Shashank! It's nice to meet you. My name is AI. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"I love Cricket\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "QUpNtb8d3HmH",
        "outputId": "5ddcec44-54b3-44df-8610-14f5d3761ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, My name is Shashank\n",
            "AI: Hello Shashank! It's nice to meet you. My name is AI. How can I assist you today?\n",
            "Human: I love Cricket\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That's great to hear, Shashank! Cricket is a popular sport played in many countries around the world. Did you know that it originated in England in the 16th century and has since become a beloved pastime for millions of people? There are many different variations of the game, including Test cricket, One Day Internationals, and Twenty20 cricket. Do you have a favorite format or team that you support?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"Yes, INDIAN Cricket Team\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "9NLbf5_O19TO",
        "outputId": "453ee891-78af-445c-80fd-fea47b35864b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: I love Cricket\n",
            "AI: That's great to hear, Shashank! Cricket is a popular sport played in many countries around the world. Did you know that it originated in England in the 16th century and has since become a beloved pastime for millions of people? There are many different variations of the game, including Test cricket, One Day Internationals, and Twenty20 cricket. Do you have a favorite format or team that you support?\n",
            "Human: Yes, INDIAN Cricket Team\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ah, the Indian cricket team is definitely a force to be reckoned with! They have a long and storied history in the sport, with many talented players over the years. Did you know that the Indian team has won the Cricket World Cup twice, in 1983 and 2011? They have also had many successful players, such as Sachin Tendulkar, Virat Kohli, and MS Dhoni. What do you think makes the Indian team so successful?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"Do you remember my name?\")   # Because k = 1, it cannot remember early exchange"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "Wns7nYXB19ZZ",
        "outputId": "640ed76b-45af-43be-c376-45f5ec0b0f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Yes, INDIAN Cricket Team\n",
            "AI: Ah, the Indian cricket team is definitely a force to be reckoned with! They have a long and storied history in the sport, with many talented players over the years. Did you know that the Indian team has won the Cricket World Cup twice, in 1983 and 2011? They have also had many successful players, such as Sachin Tendulkar, Virat Kohli, and MS Dhoni. What do you think makes the Indian team so successful?\n",
            "Human: Do you remember my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but I don't have access to that information. Could you remind me of your name?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_L5N3Hcg1hZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversational Token Buffer Memory**"
      ],
      "metadata": {
        "id": "dhpBdZLI3wUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory"
      ],
      "metadata": {
        "id": "W3IeB9wX3sh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(\n",
        "    llm = llm,\n",
        "    max_token_limit=30                 # SET MAX_TOKEN_LIMIT\n",
        ")\n",
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-emY3Ar3sfV",
        "outputId": "7b8e8e0b-2c2a-4033-dfcc-fd57ad20ef23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationTokenBufferMemory(chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', llm=ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None), memory_key='history', max_token_limit=30)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "JwDAVAD275Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Hi\"}, {\"output\":\"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much just hanging\"}, {\"output\":\"Cool\"})\n",
        "memory.save_context({\"input\": \"See you again\"}, {\"output\":\"Sure\"})"
      ],
      "metadata": {
        "id": "fZo5Cb9R3sc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})     # MAX_TOKEN_LIMIT = 30, It chops off the earlier communication"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rWjI-HB3saI",
        "outputId": "a41b0deb-00ae-4747-d094-4ab6ff893c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'AI: Cool\\nHuman: See you again\\nAI: Sure'}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OwfzmtRj8tzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversation Summary Buffer Memory**"
      ],
      "metadata": {
        "id": "53gtvMKO8_Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ],
      "metadata": {
        "id": "l_hPuO4f8tv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm = llm,\n",
        "    max_token_limit=400\n",
        ")\n",
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcNtyCjt8tsd",
        "outputId": "28f7049f-435f-466b-a106-938e6e913b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationSummaryBufferMemory(human_prefix='Human', ai_prefix='AI', llm=ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None), prompt=PromptTemplate(input_variables=['summary', 'new_lines'], output_parser=None, partial_variables={}, template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:', template_format='f-string', validate_template=True), summary_message_cls=<class 'langchain.schema.SystemMessage'>, chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=False, max_token_limit=400, moving_summary_buffer='', memory_key='history')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"\n",
        "As an ML engineer, your schedule involves tasks such as data preparation, model development, collaboration, deployment, and documentation.\n",
        "You begin with planning and reviewing emails, followed by collecting and preprocessing data, and performing exploratory data analysis.\n",
        "Model development includes designing and implementing ML models, experimenting with different approaches, and training and evaluating models.\n",
        "Collaboration and meetings with team members are essential for project progress and code reviews.\n",
        "Deployment involves optimizing models, packaging them for production, and testing in real-world scenarios.\n",
        "Documentation and reporting are necessary for knowledge sharing and communicating findings.\n",
        "Continuous learning and professional development through workshops and self-study help you stay up to date.\n",
        "Adaptability and flexibility are key in handling changing priorities.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hVtpWOrq9Dvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up?\"})\n",
        "memory.save_context({\"input\": \"Not much just hanging\"}, {\"output\":\"Cool\"})\n",
        "memory.save_context({\"input\": \"What are the tasks of ML Engineer?\"}, {\"output\":f\"{corpus}\"})"
      ],
      "metadata": {
        "id": "7LmZmMYp9DsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP8u9MsV9Dom",
        "outputId": "eb28f946-eed3-40dd-a9d0-db5bd2248dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hello\\nAI: What's up?\\nHuman: Not much just hanging\\nAI: Cool\\nHuman: What are the tasks of ML Engineer?\\nAI: \\nAs an ML engineer, your schedule involves tasks such as data preparation, model development, collaboration, deployment, and documentation. \\nYou begin with planning and reviewing emails, followed by collecting and preprocessing data, and performing exploratory data analysis.\\nModel development includes designing and implementing ML models, experimenting with different approaches, and training and evaluating models.\\nCollaboration and meetings with team members are essential for project progress and code reviews. \\nDeployment involves optimizing models, packaging them for production, and testing in real-world scenarios. \\nDocumentation and reporting are necessary for knowledge sharing and communicating findings. \\nContinuous learning and professional development through workshops and self-study help you stay up to date. \\nAdaptability and flexibility are key in handling changing priorities.\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQPHfOXiQdRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm = llm,\n",
        "    max_token_limit=100             # SET_MAX_TOKEN_LIMIT 100\n",
        ")\n",
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qvs2zsHi1hfN",
        "outputId": "dda89970-71e2-4de2-ea92-83524921439e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationSummaryBufferMemory(human_prefix='Human', ai_prefix='AI', llm=ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None), prompt=PromptTemplate(input_variables=['summary', 'new_lines'], output_parser=None, partial_variables={}, template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:', template_format='f-string', validate_template=True), summary_message_cls=<class 'langchain.schema.SystemMessage'>, chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=False, max_token_limit=100, moving_summary_buffer='', memory_key='history')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDd_RGjDQ8QI",
        "outputId": "6103ff22-40fa-4a2e-b739-25265beaded6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"System: The human greets the AI and they exchange pleasantries. The human then asks about the tasks of an ML Engineer. The AI explains that an ML Engineer's tasks include data preparation, model development, collaboration, deployment, and documentation. The AI emphasizes the importance of continuous learning and adaptability in handling changing priorities.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm = llm,\n",
        "    memory = memory,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "RIYQsN3JPScf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJj0G25CPSYi",
        "outputId": "1aa833b3-049c-4b93-ace1-243c05072c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationChain(memory=ConversationSummaryBufferMemory(human_prefix='Human', ai_prefix='AI', llm=ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None), prompt=PromptTemplate(input_variables=['summary', 'new_lines'], output_parser=None, partial_variables={}, template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:', template_format='f-string', validate_template=True), summary_message_cls=<class 'langchain.schema.SystemMessage'>, chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=False, max_token_limit=100, moving_summary_buffer='', memory_key='history'), callbacks=None, callback_manager=None, verbose=True, tags=None, prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True), llm=ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None), output_key='response', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}, input_key='input')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"What are the tasks of ML Engineer?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "AEipOAV6RNaM",
        "outputId": "a1ebb872-3eb4-4ec3-8c05-543dba30bae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI and they exchange pleasantries. The human then asks about the tasks of an ML Engineer. The AI explains that an ML Engineer's tasks include data preparation, model development, collaboration, deployment, and documentation. The AI emphasizes the importance of continuous learning and adaptability in handling changing priorities.\n",
            "Human: What are the tasks of ML Engineer?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"An ML Engineer's tasks include data preparation, model development, collaboration, deployment, and documentation. It's important to note that continuous learning and adaptability are also crucial in handling changing priorities.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp1pJqzbRNXG",
        "outputId": "dedc4bdb-ed06-4861-d06d-51a2755d21c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"System: The human greets the AI and they exchange pleasantries. The human then asks about the tasks of an ML Engineer. The AI explains that an ML Engineer's tasks include data preparation, model development, collaboration, deployment, and documentation. The AI emphasizes the importance of continuous learning and adaptability in handling changing priorities.\\nHuman: What are the tasks of ML Engineer?\\nAI: An ML Engineer's tasks include data preparation, model development, collaboration, deployment, and documentation. It's important to note that continuous learning and adaptability are also crucial in handling changing priorities.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QcTcy6gzRNTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Data Memory, Entity Memories**"
      ],
      "metadata": {
        "id": "W_NJDPvfSrBG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4q7IxRjZSqit"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}